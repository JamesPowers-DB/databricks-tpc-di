{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66526744-2773-4893-948d-db36d44ea8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08a7e416-fb75-4a95-be9b-6534ace81af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"scale_factor\", \"10\", [\"10\", \"100\", \"1000\", \"5000\", \"10000\", \"20000\"])\n",
    "dbutils.widgets.text(\"existing_files_directory\", \"/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/\")\n",
    "dbutils.widgets.text(\"parquet_files_directory\", \"/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/splitparquet\")\n",
    "dbutils.widgets.text(\"catalog\", \"tpcdi\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "scale_factor = dbutils.widgets.get(\"scale_factor\")\n",
    "tpcdi_directory = dbutils.widgets.get(\"existing_files_directory\")\n",
    "staging_dir = dbutils.widgets.get(\"parquet_files_directory\") + f\"/sf={scale_factor}\"\n",
    "\n",
    "tables_ls = [\n",
    "    [\"HR\", 16, \"csv\", \"employeeid BIGINT, managerid BIGINT, employeefirstname STRING, employeelastname STRING, employeemi STRING, employeejobcode STRING , employeebranch STRING, employeeoffice STRING, employeephone STRING\"],\n",
    "    [\"CashTransaction\", 400, \"txt\", \"accountid BIGINT, ct_dts TIMESTAMP, ct_amt DOUBLE, ct_name STRING\"],\n",
    "    [\"DailyMarket\", 600, \"txt\", \"dm_date DATE, dm_s_symb STRING, dm_close DOUBLE, dm_high DOUBLE, dm_low DOUBLE, dm_vol INT\"],\n",
    "    [\"HoldingHistory\", 60, \"txt\", \"hh_h_t_id BIGINT, hh_t_id BIGINT, hh_before_qty INT, hh_after_qty INT\"],\n",
    "    [\"Prospect\", 40, \"csv\", \"agencyid STRING, lastname STRING, firstname STRING, middleinitial STRING, gender STRING, addressline1 STRING, addressline2 STRING, postalcode STRING, city STRING, state STRING, country STRING, phone STRING, income STRING, numbercars STRING, numberchildren STRING, maritalstatus STRING, age STRING, creditrating STRING, ownorrentflag STRING, employer STRING, numbercreditcards STRING, networth STRING\"],\n",
    "    [\"TradeHistory\", 200, \"txt\", \"tradeid BIGINT, th_dts TIMESTAMP, status STRING\"],\n",
    "    [\"Trade\", 300, \"txt\", \"t_id BIGINT, t_dts TIMESTAMP, t_st_id STRING, t_tt_id STRING, t_is_cash TINYINT, t_s_symb STRING, quantity INT, bidprice DOUBLE, t_ca_id BIGINT, executedby STRING, tradeprice DOUBLE, fee DOUBLE, commission DOUBLE, tax DOUBLE\"],\n",
    "    [\"WatchHistory\", 300, \"txt\", \"w_c_id BIGINT, w_s_symb STRING, w_dts TIMESTAMP, w_action STRING\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c816d52-d4c7-4498-804a-b1310b534236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rewrite_files(file_cnt, delimiter, tbl, filetype, raw_schema, batchnum):\n",
    "  df = spark.sql(f\"\"\"\n",
    "  select * FROM read_files(\n",
    "    \"{tpcdi_directory}sf={scale_factor}/Batch{batchnum}/\",\n",
    "    format => \"csv\",\n",
    "    inferSchema => False,\n",
    "    header => False,\n",
    "    sep => \"{delimiter}\",\n",
    "    fileNamePattern => \"{tbl}.{filetype}\",\n",
    "    schema => \"{raw_schema}\"\n",
    "  )\n",
    "  \"\"\")\n",
    "  df.repartition(file_cnt).write.mode(\"overwrite\").parquet(f\"{staging_dir}/_stage/Batch{batchnum}/{tbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a77a00f-ab09-439b-9ba7-9ee479f5571d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threads = len(tables_ls)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "  futures = []\n",
    "  for tbl in tables_ls:\n",
    "    delimiter = ',' if tbl[2] == \"csv\" else '|'\n",
    "    futures.append(executor.submit(rewrite_files, file_cnt=tbl[1], delimiter=delimiter, tbl=tbl[0], filetype=tbl[2], raw_schema=tbl[3], batchnum=1))\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    try: print(future.result())\n",
    "    except requests.ConnectTimeout: print(\"ConnectTimeout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1b20f7-752c-41ed-aa0b-a183ef52fbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def move_file(src_loc, tgt_loc):\n",
    "  dbutils.fs.cp(src_loc, tgt_loc)\n",
    "  # dbutils.fs.rm(src_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c094756-d203-4c66-ab94-957203848d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files_map = []\n",
    "for tbl in tables_ls:\n",
    "  file_num = 1\n",
    "  for file in glob(f\"{staging_dir}/_stage/Batch1/{tbl[0]}/part-00*.parquet\"):\n",
    "    files_map.append([file, f\"{staging_dir}/Batch1/{tbl[0]}_{file_num}.parquet\"])\n",
    "    file_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00fe5800-ecc2-4f69-8baf-866b92b1e64f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threads = 64\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "  futures = []\n",
    "  for file in files_map:\n",
    "    futures.append(executor.submit(move_file, src_loc=file[0], tgt_loc=file[1]))\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    try: print(future.result())\n",
    "    except requests.ConnectTimeout: print(\"ConnectTimeout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308690f9-b5be-4f25-ba2c-6b9528b60ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables_ls = [\n",
    "  [1, 'BatchDate', 'batchdate DATE'],\n",
    "  [1, 'Date', 'sk_dateid BIGINT, datevalue DATE, datedesc STRING, calendaryearid INT, calendaryeardesc STRING, calendarqtrid INT, calendarqtrdesc STRING, calendarmonthid INT, calendarmonthdesc STRING, calendarweekid INT, calendarweekdesc STRING, dayofweeknum INT, dayofweekdesc STRING, fiscalyearid INT, fiscalyeardesc STRING, fiscalqtrid INT, fiscalqtrdesc STRING, holidayflag BOOLEAN'],\n",
    "  [1, 'Industry', 'in_id STRING, in_name STRING, in_sc_id STRING'],\n",
    "  [1, 'StatusType', 'st_id STRING, st_name STRING'],\n",
    "  [1, 'TaxRate', 'tx_id STRING, tx_name STRING, tx_rate FLOAT'],\n",
    "  [1, 'Time', 'sk_timeid BIGINT, timevalue STRING, hourid INT, hourdesc STRING, minuteid INT, minutedesc STRING, secondid INT, seconddesc STRING, markethoursflag BOOLEAN, officehoursflag BOOLEAN'],\n",
    "  [1, 'TradeType', 'tt_id STRING, tt_name STRING, tt_is_sell INT, tt_is_mrkt INT'],\n",
    "  [2, 'Account', 'cdc_flag STRING, cdc_dsn BIGINT, accountid BIGINT, brokerid BIGINT, customerid BIGINT, accountdesc STRING, taxstatus TINYINT, status STRING'],\n",
    "  [2, 'BatchDate', 'batchdate DATE'],\n",
    "  [2, 'CashTransaction', 'cdc_flag STRING, cdc_dsn BIGINT, accountid BIGINT, ct_dts TIMESTAMP, ct_amt DOUBLE, ct_name STRING'],\n",
    "  [2, 'Customer', 'cdc_flag STRING, cdc_dsn BIGINT, customerid BIGINT, taxid STRING, status STRING, lastname STRING, firstname STRING, middleinitial STRING, gender STRING, tier TINYINT, dob DATE, addressline1 STRING, addressline2 STRING, postalcode STRING, city STRING, stateprov STRING, country STRING, c_ctry_1 STRING, c_area_1 STRING, c_local_1 STRING, c_ext_1 STRING, c_ctry_2 STRING, c_area_2 STRING, c_local_2 STRING, c_ext_2 STRING, c_ctry_3 STRING, c_area_3 STRING, c_local_3 STRING, c_ext_3 STRING, email1 STRING, email2 STRING, lcl_tx_id STRING, nat_tx_id STRING'],\n",
    "  [2, 'DailyMarket', 'cdc_flag STRING, cdc_dsn BIGINT, dm_date DATE, dm_s_symb STRING, dm_close DOUBLE, dm_high DOUBLE, dm_low DOUBLE, dm_vol INT'],\n",
    "  [2, 'HoldingHistory', 'cdc_flag STRING, cdc_dsn BIGINT, hh_h_t_id BIGINT, hh_t_id BIGINT, hh_before_qty INT, hh_after_qty INT'],\n",
    "  [2, 'Trade', 'cdc_flag STRING, cdc_dsn BIGINT, tradeid BIGINT, t_dts TIMESTAMP, status STRING, t_tt_id STRING, cashflag TINYINT, t_s_symb STRING, quantity INT, bidprice DOUBLE, t_ca_id BIGINT, executedby STRING, tradeprice DOUBLE, fee DOUBLE, commission DOUBLE, tax DOUBLE'],\n",
    "  [2, 'WatchHistory', 'cdc_flag STRING, cdc_dsn BIGINT, w_c_id BIGINT, w_s_symb STRING, w_dts TIMESTAMP, w_action STRING'],\n",
    "  [3, 'Account','cdc_flag STRING, cdc_dsn BIGINT, accountid BIGINT, brokerid BIGINT, customerid BIGINT, accountdesc STRING, taxstatus TINYINT, status STRING'],\n",
    "  [3, 'BatchDate', 'batchdate DATE'],\n",
    "  [3, 'CashTransaction', 'cdc_flag STRING, cdc_dsn BIGINT, accountid BIGINT, ct_dts TIMESTAMP, ct_amt DOUBLE, ct_name STRING'],\n",
    "  [3, 'Customer', 'cdc_flag STRING, cdc_dsn BIGINT, customerid BIGINT, taxid STRING, status STRING, lastname STRING, firstname STRING, middleinitial STRING, gender STRING, tier TINYINT, dob DATE, addressline1 STRING, addressline2 STRING, postalcode STRING, city STRING, stateprov STRING, country STRING, c_ctry_1 STRING, c_area_1 STRING, c_local_1 STRING, c_ext_1 STRING, c_ctry_2 STRING, c_area_2 STRING, c_local_2 STRING, c_ext_2 STRING, c_ctry_3 STRING, c_area_3 STRING, c_local_3 STRING, c_ext_3 STRING, email1 STRING, email2 STRING, lcl_tx_id STRING, nat_tx_id STRING'],\n",
    "  [3, 'DailyMarket', 'cdc_flag STRING, cdc_dsn BIGINT, dm_date DATE, dm_s_symb STRING, dm_close DOUBLE, dm_high DOUBLE, dm_low DOUBLE, dm_vol INT'],\n",
    "  [3, 'HoldingHistory', 'cdc_flag STRING, cdc_dsn BIGINT, hh_h_t_id BIGINT, hh_t_id BIGINT, hh_before_qty INT, hh_after_qty INT'],\n",
    "  [3, 'Trade', 'cdc_flag STRING, cdc_dsn BIGINT, tradeid BIGINT, t_dts TIMESTAMP, status STRING, t_tt_id STRING, cashflag TINYINT, t_s_symb STRING, quantity INT, bidprice DOUBLE, t_ca_id BIGINT, executedby STRING, tradeprice DOUBLE, fee DOUBLE, commission DOUBLE, tax DOUBLE'],\n",
    "  [3, 'WatchHistory', 'cdc_flag STRING, cdc_dsn BIGINT, w_c_id BIGINT, w_s_symb STRING, w_dts TIMESTAMP, w_action STRING']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7920e70e-2c0a-42e1-8cf2-90ecec8d69fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threads = len(tables_ls)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "  futures = []\n",
    "  for tbl in tables_ls:\n",
    "    futures.append(executor.submit(rewrite_files, file_cnt=1, delimiter='|', tbl=tbl[1], filetype='txt', raw_schema=tbl[2], batchnum=tbl[0]))\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    try: print(future.result())\n",
    "    except requests.ConnectTimeout: print(\"ConnectTimeout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15755efa-1858-498b-a1ac-0a2644ab4416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files_map = []\n",
    "for dir in tables_ls:\n",
    "  for file in glob(f\"{staging_dir}/_stage/Batch{dir[0]}/{dir[1]}/part-00*.parquet\"):\n",
    "    files_map.append([file, f\"{staging_dir}/Batch{dir[0]}/{dir[1]}.parquet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72d96e7d-983b-4090-8d27-0c67621b3d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threads = 64\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "  futures = []\n",
    "  for file in files_map:\n",
    "    futures.append(executor.submit(move_file, src_loc=file[0], tgt_loc=file[1]))\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    try: print(future.result())\n",
    "    except requests.ConnectTimeout: print(\"ConnectTimeout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b82edd4b-645f-487c-8007-66b84bedbed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(f\"select * FROM text.`{tpcdi_directory}sf={scale_factor}/Batch1/FINWIRE[0-9][0-9][0-9][0-9]Q[1-4]`\")\n",
    "df.repartition(400).write.mode(\"overwrite\").parquet(f\"{staging_dir}/_stage/Batch1/FINWIRE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b647a4d-cef0-4897-88a5-54655515bbfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files_map = []\n",
    "file_num = 1\n",
    "for file in glob(f\"{staging_dir}/_stage/Batch1/FINWIRE/part-00*.parquet\"):\n",
    "  files_map.append([file, f\"{staging_dir}/Batch1/FINWIRE_{file_num}.parquet\"])\n",
    "  file_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d12a7b7b-8d08-4c3b-8190-99d0a8c60def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threads = 64\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "  futures = []\n",
    "  for file in files_map:\n",
    "    futures.append(executor.submit(move_file, src_loc=file[0], tgt_loc=file[1]))\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    try: print(future.result())\n",
    "    except requests.ConnectTimeout: print(\"ConnectTimeout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526f6771-ec13-4544-ba5b-2f428a802a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threads = 2\n",
    "tbl = [\n",
    "    \"Prospect\", 40, \"csv\", \"agencyid STRING, lastname STRING, firstname STRING, middleinitial STRING, gender STRING, addressline1 STRING, addressline2 STRING, postalcode STRING, city STRING, state STRING, country STRING, phone STRING, income STRING, numbercars STRING, numberchildren STRING, maritalstatus STRING, age STRING, creditrating STRING, ownorrentflag STRING, employer STRING, numbercreditcards STRING, networth STRING\"    \n",
    "]\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "  futures = []\n",
    "  for batchnum in range(2, 4):\n",
    "    futures.append(executor.submit(rewrite_files, file_cnt=tbl[1], delimiter=',', tbl=tbl[0], filetype=tbl[2], raw_schema=tbl[3], batchnum=batchnum))\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    try: print(future.result())\n",
    "    except requests.ConnectTimeout: print(\"ConnectTimeout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed27982b-e67b-4a7c-8aa5-563826661679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "files_map = []\n",
    "for batchnum in range(2, 4):\n",
    "  file_num = 1\n",
    "  for file in glob(f\"{staging_dir}/_stage/Batch{batchnum}/{tbl[0]}/part-00*.parquet\"):\n",
    "    files_map.append([file, f\"{staging_dir}/Batch{batchnum}/{tbl[0]}_{file_num}.parquet\"])\n",
    "    file_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad49215a-8e36-425d-8439-046648b47079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "threads = len(files_map)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "  futures = []\n",
    "  for file in files_map:\n",
    "    futures.append(executor.submit(move_file, src_loc=file[0], tgt_loc=file[1]))\n",
    "  for future in concurrent.futures.as_completed(futures):\n",
    "    try: print(future.result())\n",
    "    except requests.ConnectTimeout: print(\"ConnectTimeout.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1d215d-6d23-4519-a63d-2de13f7d013e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_cnt = 0\n",
    "expected_file_cnt = 2323\n",
    "\n",
    "for file in glob(f\"{staging_dir}/Batch1/*.parquet\"):\n",
    "  file_cnt += 1\n",
    "\n",
    "print('Total Converted Parquet files: ' + str(file_cnt))\n",
    "\n",
    "if file_cnt != expected_file_cnt:\n",
    "  print('Number of Parquet files does not match the expected amount of ' + str(expected_file_cnt))\n",
    "else:\n",
    "  print('Number of Parquet files matches the expected amount of ' + str(expected_file_cnt))\n",
    "  dbutils.fs.rm(f\"{staging_dir}/_stage\", recurse=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "parquet_splitting",
   "widgets": {
    "catalog": {
     "currentValue": "tpcdi",
     "nuid": "fb8c4a9c-21c8-497a-8314-0bc92cfe2b44",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tpcdi",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tpcdi",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "existing_files_directory": {
     "currentValue": "/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/",
     "nuid": "92205d01-a765-4ba2-a3e8-dfd3ffd6a171",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/",
      "label": null,
      "name": "existing_files_directory",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/",
      "label": null,
      "name": "existing_files_directory",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "parquet_files_directory": {
     "currentValue": "/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/splitparquet",
     "nuid": "0ac1b398-52bd-48a6-80dc-9a53629d64fa",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/splitparquet",
      "label": null,
      "name": "parquet_files_directory",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/tpcdi/tpcdi_raw_data/tpcdi_volume/splitparquet",
      "label": null,
      "name": "parquet_files_directory",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "scale_factor": {
     "currentValue": "10000",
     "nuid": "a92ecbdd-e3f4-4212-a012-ed2a7691d416",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": null,
      "name": "scale_factor",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "10",
        "100",
        "1000",
        "5000",
        "10000",
        "20000"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "10",
      "label": null,
      "name": "scale_factor",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "10",
        "100",
        "1000",
        "5000",
        "10000",
        "20000"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
